{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The aim of this project is to create a retrieval based ChatBot using predefined patterns and responses.\n",
    "\n",
    "Table of contents:<br>\n",
    "* [1.Import and Load](#importandload)\n",
    "* [2.Preprocess](#preprocess)\n",
    "* [3.Train and Test Data](#traintest)\n",
    "* [4.RNN](#model)\n",
    "* [5.Graphical User Interface and Prediction](#gui)\n",
    "\n",
    "###### Team members: Name (Registration Number):\n",
    "###### Vakouftsis_Athanasios  (2022202004002), Tsarouchi Alexandra(2022202004024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Import and Load\n",
    "<a id=\"importandload\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data we used are saved in intents.json file and contains intents like the one below\n",
    "    {\"tag\": \"greeting\",\n",
    "         \"patterns\": [\"Hi there\", \"How are you\", \"Is anyone there?\",\"Hey\",\"Hola\", \"Hello\", \"Good day\"],\n",
    "         \"responses\": [\"Hello, what do you want to know?\", \"Good to see you again, what would you like to know?\", \"Hi there, how can I help?\"],\n",
    "         \"context\": [\"\"]\n",
    "    }\n",
    "#Where tag describes the type of intent, patterns the type of question the user want to ask and responses describes the one\n",
    "#random response the bot will give. So we import the json file with:\n",
    "\n",
    "intents = json.loads(open('intents.json').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Preprocess\n",
    "<a id=\"preprocess\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As for the preprocess of the data we tokenize each word and add it to a list of words and then we add each word in a document\n",
    "#with the tag that this word has and we add each class on a list.Below we can see the code:\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        #Tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #Add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        #Add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "#After, we lemmatize every word with lemmatizer and we save the words and the classes on pickle files respectively.\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Train and Test Data\n",
    "<a id=\"traintest\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create our training data with the code below:\n",
    "for doc in documents:\n",
    "    #Initialize our bag of words\n",
    "    bag = []\n",
    "    #List of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    #Lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    #Create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    #Output is a '0' for each tag and '1' for each pattern\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "\n",
    "#Then we create two lists train_x(containing all patterns with all the words) and train_y(containing all patterns with all the intents)\n",
    "#and we divide the 90% of the data to x_train, y_train and 10% of the data to x_test,y_test for our model and the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RNN\n",
    "<a id=\"model\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We used a simple RNN and not an LSTM because we had very good accuracy(around 96% and on some occasions up to 100%). We found\n",
    "#that with the baseline of a RNN(one input layer,one hidden layer and one output layer) didn't get the desirable results so\n",
    "#we used 3 layers and 2 hidden layers. We found that 80 neurons for the input layer was the sweet spot between accuracy and not\n",
    "#overfitting the model. The output layer has the length of the y_train which is the number of classes used for training. Finally\n",
    "#we used SGD optimizer and nesterov for the best possible results. Below we can see the mdoel:\n",
    "model = Sequential()\n",
    "model.add(Dense(80, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "#Compile model with Stochastic gradient descent with Nesterov the best possible results\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Graphical User Interface and Prediction\n",
    "<a id=\"gui\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below we can see the function in which we predict the class of the users response:\n",
    "def predict_class(sentence, model):\n",
    "    #Filter out predictions below a threshold\n",
    "    p = bow(sentence, words, show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    #Sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "#Then we created an interface in order to communicate with the ChatBot. We used tkinter library to create the interface and the\n",
    "#user can either press the send button to send the message or press the ENTER key of the keyboard.\n",
    "#For more information there is all the code in the python files, we used PyCharm to create the ChatBot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
